{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('[{\"business_id\":\"--9QQLMTbFzLJ_oT-ON3Xw\",\"date\":\"2010-06-26 17:39:07, 2010-08-01 20:06:21, 2010-12-09 21:21:24\"},{\"business_id\":\"-7zmmkVg-IMGaXbuVd0SQ\",\"date\":\"2014-12-29 19:25:50, 2015-01-17 01:49:14\"}]', orient = 'records')\n",
    "#df = pd.read_json(r'\\Users\\vcroopana\\Downloads\\Spring2020\\BigData\\Proj1_No_SQL\\yelp_dataset\\samplecheckin.json', orient = 'records')\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    cluster = Cluster(['0.0.0.0'], port = 9042)\n",
    "    session = cluster.connect('json',wait_for_all_pools=True) #json - name of keyspace\n",
    "    session.execute('USE json')\n",
    "\n",
    "  #  query = \"INSERT INTO review(reviewId,userId,businessId,rating,reviewDate) VALUES (?,?,?,?,?)\"\n",
    "    query = \"INSERT INTO checkin(businessid,dates) VALUES (?,?)\"\n",
    "    prepared = session.prepare(query)\n",
    "    for index, item in df.iterrows():\n",
    "        #print(item)\n",
    "        #session.execute(prepared, (item['review_id'],item['user_id'],item['business_id'],str(item['stars']),str(item['date'])))\n",
    "        session.execute(prepared, (item['business_id'],str(item['date'])))\n",
    "    \n",
    "    rows = session.execute('SELECT * FROM checkin')\n",
    "    for row in rows:\n",
    "        #print(row)\n",
    "        print(\"INSERT INTO checkin(businessid,dates) VALUES (\",\"'\"+row.businessid+\"'\", \",'\"+row.dates+\"'\",\");\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df = pd.read_json(r'\\Users\\vcroopana\\Downloads\\Spring2020\\BigData\\Proj1_No_SQL\\samplereview.json')\n",
    "df = pd.read_json('[{\"review_id\":\"RS_GTIT6836bCaPy637kNQ\",\"user_id\":\"nMeCE5-xsdleyxYuNZ_7rA\",\"business_id\":\"oxwGyA17NL6c5t1Etg5WgQ\",\"stars\":3.0,\"useful\":1,\"funny\":0,\"cool\":1,\"text\":\"It s a giant Best Buy with 66 registers.  I dont get it.  What s the big deal about this place??\",\"date\":\"2012-02-29 21:52:43\"},{\"review_id\":\"kbtscdyz6lvrtGjD1quQTg\",\"user_id\":\"FIk4lQQu1eTe2EpzQ4xhBA\",\"business_id\":\"8mIrX_LrOnAqWsB5JrOojQ\",\"stars\":4.0,\"useful\":0,\"funny\":0,\"cool\":0,\"text\":\"Like walking back in time, every Saturday morning my sister and I was in a bowling league and after we were done, we d spend a few quarters playing the pin ball machines until our mother came to pick us up.\\n\\nMy sister was daring and play the machines hard, she was afraid of that tilt showing up and freezing the game.  I, on the other hand was a bit more gentler and wanted to make sure I got my quarter s worth.This place has rows and rows of machines, some are really old and some are more of a mid 80 s theme.  There is even a Ms pac man!  It was fun to spend an afternoon playing the machines and remembering all the fun of my early teen years.\",\"date\":\"2011-11-30 02:11:15\"},{\"review_id\":\"-I5umRTkhw15RqpKMl_o1Q\",\"user_id\":\"-mA3-1mN4JIEkqOtdbNXCQ\",\"business_id\":\"mRUVMJkUGxrByzMQ2MuOpA\",\"stars\":1.0,\"useful\":0,\"funny\":1,\"cool\":0,\"text\":\"Walked in around 4 on a Friday afternoon, we sat at a table just off the bar and walked out after 5 min or so. Don t even think they realized we walked in. However everyone at the bar noticed we walked in!!! Service was non existent at best. Not a good way for a new business to start out. Oh well, the location they are at has been about 5 different things over the past several years, so they will just be added to the list. SMDH!!!\",\"date\":\"2017-12-15 23:27:08\"}]', orient = 'records')\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "import pandas as pd\n",
    "\n",
    "#df = pd.read_json('[{\"review_id\":\"RS_GTIT6836bCaPy637kNQ\",\"user_id\":\"nMeCE5-xsdleyxYuNZ_7rA\",\"business_id\":\"oxwGyA17NL6c5t1Etg5WgQ\",\"stars\":3.0,\"useful\":1,\"funny\":0,\"cool\":1,\"text\":\"It s a giant Best Buy with 66 registers.  I dont get it.  What s the big deal about this place??\",\"date\":\"2012-02-29 21:52:43\"},{\"review_id\":\"kbtscdyz6lvrtGjD1quQTg\",\"user_id\":\"FIk4lQQu1eTe2EpzQ4xhBA\",\"business_id\":\"8mIrX_LrOnAqWsB5JrOojQ\",\"stars\":4.0,\"useful\":0,\"funny\":0,\"cool\":0,\"text\":\"Like walking back in time, every Saturday morning my sister and I was in a bowling league and after we were done, we d spend a few quarters playing the pin ball machines until our mother came to pick us up.\\n\\nMy sister was daring and play the machines hard, she was afraid of that tilt showing up and freezing the game.  I, on the other hand was a bit more gentler and wanted to make sure I got my quarter s worth.This place has rows and rows of machines, some are really old and some are more of a mid 80 s theme.  There is even a Ms pac man!  It was fun to spend an afternoon playing the machines and remembering all the fun of my early teen years.\",\"date\":\"2011-11-30 02:11:15\"},{\"review_id\":\"-I5umRTkhw15RqpKMl_o1Q\",\"user_id\":\"-mA3-1mN4JIEkqOtdbNXCQ\",\"business_id\":\"mRUVMJkUGxrByzMQ2MuOpA\",\"stars\":1.0,\"useful\":0,\"funny\":1,\"cool\":0,\"text\":\"Walked in around 4 on a Friday afternoon, we sat at a table just off the bar and walked out after 5 min or so. Don t even think they realized we walked in. However everyone at the bar noticed we walked in!!! Service was non existent at best. Not a good way for a new business to start out. Oh well, the location they are at has been about 5 different things over the past several years, so they will just be added to the list. SMDH!!!\",\"date\":\"2017-12-15 23:27:08\"},{\"review_id\":\"qlXw1JQ0UodW7qrmVgwCXw\",\"user_id\":\"bAhqAPoWaZYcyYi7bs024Q\",\"business_id\":\"LUN6swQYa4xJKaM_UEUOEw\",\"stars\":4.0,\"useful\":0,\"funny\":0,\"cool\":0,\"text\":\"Michael from Red Carpet VIP is amazing ! I reached out because I needed help planning my soon to be sister in law s bachelorette. It was a group of 10 girls so I was a little overwhelmed but Michael saved the day! Everything was super smooth and easy! We got good deals and had the best time ever! We booked hotel and a bachelorette package for a great price. I have saved contact info because I will for sure reach out again on next Vegas trip!!!\",\"date\":\"2018-04-27 20:25:26\"},{\"review_id\":\"JVcjMhlavKKn3UIt9p9OXA\",\"user_id\":\"TpyOT5E16YASd7EWjLQlrw\",\"business_id\":\"AakkkTuGZA2KBodKi2_u8A\",\"stars\":1.0,\"useful\":1,\"funny\":1,\"cool\":0,\"text\":\"I cannot believe how things have changed in 3 years. I picked up duck congee sometime in the winter when my hubby was sick.  I was very disappointed because the ginger fish sauce tasted like it had gone bad (it should never be bitter).  Today, my hubby wanted to eat there since he was craving the duck congee and most places don t serve the duck & coleslaw side. We waited about 10 minutes to get our menu.  After we placed our orders, we waited another 5 minutes to get the tea that most places bring with the menu.  I could go on with the details but the gist of the story is they were understaffed or the staff was slow.  The worst part of it was that the service.  The servers make us feel bad for asking for anything (like when they took our order).  We had arrived and placed our order before another couple bside us at least 10 minutes ahead but somehow, this couple received their pho before mine.  They were almost done eating their pho before mine came out.\",\"date\":\"2012-07-16 00:37:14\"},{\"review_id\":\"svK3nBU7Rk8VfGorlrN52A\",\"user_id\":\"NJlxGtouq06hhC7sS2ECYw\",\"business_id\":\"YvrylyuWgbP90RgMqZQVnQ\",\"stars\":5.0,\"useful\":0,\"funny\":0,\"cool\":0,\"text\":\"You can t really find anything wrong with this place, the pastas and pizzas are both amazing and high quality, the price is very reasonable, the owner and the staff are very friendly, if you re in downtown check this place out, a lot of people think just because it s downtown there are lots of options around but that s not always the case as there is also a lot of poor quality food in downtown as well.\",\"date\":\"2017-04-07 21:27:49\"},{\"review_id\":\"1wVA2-vQIuW_ClmXkDxqMQ\",\"user_id\":\"86J5DwcFk4f4In1Vxe2TvA\",\"business_id\":\"NyLYY8q1-H3hfsTwuwLPCg\",\"stars\":4.0,\"useful\":0,\"funny\":0,\"cool\":0,\"text\":\"Great lunch today. Staff was very helpful in assisting with selections and knowledgeable on the ingredients. We enjoyed the BBQ chicken with tika masala sauce and really good naan bread. The biryani with chicken was also yummy! Fun to see the food being prepared in the tandoori ovens. Great addition to the fast casual scene in Cleveland.\",\"date\":\"2015-01-03 22:47:34\"},{\"review_id\":\"j3vP8537KHvoXNHQIr3haA\",\"user_id\":\"Z_HE_KKT7N-WddPTzUQC7A\",\"business_id\":\"jScBTQtdAt-8RshaiBEHgw\",\"stars\":5.0,\"useful\":1,\"funny\":0,\"cool\":1,\"text\":\"Party of 3 ordered the fish tacos, pork belly banh mi, corn beef hash Benedict, and bam ham. Everything was delicious, esp the pork belly banh mi. It s tender and juicy and the baguette was nicely toasted. Love the crispy fish taco too. Their compliment chips and salsa were fresh and super good and crispy. Service was excellent. Our waitress did a great job checking up on us and serving us. I really want other food options too but I m too full. Next time i visit i will definitely come back.\",\"date\":\"2018-05-28 20:56:05\"},{\"review_id\":\"f0B9-r14-bLudyu5S7aLhw\",\"user_id\":\"SvMGr_Oih7ivtzmnCL-Tmg\",\"business_id\":\"ujHiaprwCQ5ewziu0Vi9rw\",\"stars\":1.0,\"useful\":1,\"funny\":0,\"cool\":0,\"text\":\"We had dinner at the Bellagio Buffet last night. The service was OK. Our server was great but kind of forgot about us towards the end of our visit. The food was cold. The only good thing there was the crab legs because they re suppose to be cold and the tacos because it was freshly made. The food was under a single lamp that didn t heat the food at all. We let the server know that the food was cold and he told the manager that was in charge. All she did was come over to ask what the problem was and said she was going to talk to the chief. Have no idea what actually happened though because we didn t see her after that. Half the buffet was closed as well so there wasn t that many options. I must say i rather go to the Rio buffet. For a high end casino this was a horrible experience.\",\"date\":\"2013-12-07 00:14:06\"}]', orient = 'records')\n",
    "df = pd.read_json('[{\"business_id\":\"--9QQLMTbFzLJ_oT-ON3Xw\",\"date\":\"2010-06-26 17:39:07, 2010-08-01 20:06:21, 2010-12-09 21:21:24\"},{\"business_id\":\"-7zmmkVg-IMGaXbuVd0SQ\",\"date\":\"2014-12-29 19:25:50, 2015-01-17 01:49:14\"}]', orient = 'records')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #cluster = Cluster(['ec2-52-87-203-221.compute-1.amazonaws.com'],port=9160)\n",
    "    cluster = Cluster(['0.0.0.0'], port = 9042)\n",
    "    session = cluster.connect('json',wait_for_all_pools=True) #json - name of keyspace\n",
    "    session.execute('USE json')\n",
    "\n",
    "  # query = \"INSERT INTO review(reviewId,userId,businessId,rating,reviewDate) VALUES (?,?,?,?,?)\"\n",
    "    query = \"INSERT INTO checkin(businessid,dates) VALUES (?,?)\"\n",
    "    prepared = session.prepare(query)\n",
    "    for index, item in df.iterrows():\n",
    "\n",
    "        #session.execute(prepared, (item['review_id'],item['user_id'],item['business_id'],str(item['stars']),str(item['date'])))\n",
    "        session.execute(prepared, (item['business_id'],str(item['date'])))\n",
    "    \n",
    "    rows = session.execute('SELECT * FROM checkin')\n",
    "    for row in rows:\n",
    "        print(\"INSERT INTO checkin(businessid,dates) VALUES (\",\"'\"+row.businessid+\"'\", \",'\"+row.dates+\"'\",\");\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#testing replace operations\n",
    "import os\n",
    "print(os. getcwd())\n",
    "#NotebookApp.iopub_data_rate_limit = 10000000\n",
    "file = open(r\"/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/tip.json\",\"r\")\n",
    "stri = file.read()\n",
    "stri = stri.replace('}{', '},{') \n",
    "stri = '[' + stri + ']' \n",
    "print(stri)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#user, review, business, tip\n",
    "\n",
    "fin = open(\"/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/user.json\", \"r\")\n",
    "fout = open(\"/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/usero.json\", \"wt\")\n",
    "\n",
    "replacedLine = \"[\"\n",
    "prevOriginalLine = \"\"\n",
    "\n",
    "for line in fin:\n",
    "    fout.write(replacedLine) # Adds [ to the first line and then replaced line in subsequent iterations\n",
    "    prevOriginalLine = line\n",
    "    replacedLine = line.replace('}\\n', '},\\n')\n",
    "        \n",
    "fout.write( prevOriginalLine + \"]\") # Appends ] to the last line\n",
    "\n",
    "fin.close()\n",
    "fout.close()\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/businesso.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(\"No of rows:\"+ str(len(data)))\n",
    "    print(data[0])\n",
    "    for p in data:\n",
    "        #print('business_id: ' + p['business_id'])\n",
    "        if 'categories' in p and p['categories'] is None:\n",
    "            #print('categories: ' + 'None')\n",
    "        else:\n",
    "            continue\n",
    "            #print('categories: ' + p['categories'])\n",
    "        \n",
    "        print('')\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "pip install ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mpu\n",
    "def distance(point1, point2):\n",
    "    return mpu.haversine_distance(point1, point2)\n",
    "\n",
    "def closest(data, this_point):\n",
    "    return min(data, key = lambda x: distance(this_point, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/MartinThoma/mpu.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def extractToFiles(header, inputFile, outputFilePath, outputFilePrefix, fields):\n",
    "    parser = ijson.parse(open(inputFile, 'r'))\n",
    "    filecount = 1\n",
    "    writefile= open(outputFilePath+outputFilePrefix+\"1.txt\",\"w+\")\n",
    "    writefile.writelines(header)\n",
    "\n",
    "    count = 1\n",
    "    print(\"count: \"+ str(count) + \" curr filecount: \"+ str(filecount))\n",
    "\n",
    "    for prefix, event, value in parser:\n",
    "        for i in range(0, len(fields)):\n",
    "            if prefix.endswith(fields[i]): \n",
    "                if count == (filecount* 100000) and i==0 : # change to new file for every 1 lakh rows\n",
    "                    writefile.close()\n",
    "                    filecount = filecount+1\n",
    "                    print(\"count: \"+ str(count) + \" curr filecount: \"+ str(filecount))\n",
    "                    writefile= open(outputFilePath+outputFilePrefix+str(filecount)+\".txt\",\"w+\")       \n",
    "                    writefile.writelines(header)\n",
    "                if value is not None:\n",
    "                    value = value.replace('@', '||')\n",
    "                writefile.writelines(str(value))\n",
    "                if i!= len(fields)-1 : # if not the end of row append with @\n",
    "                    writefile.writelines(\"@\") \n",
    "                else:                   # if end of row print new line char\n",
    "                    writefile.writelines(\"\\n\")\n",
    "                if i==0: count = count+1  # increment the count for each row\n",
    "                \n",
    "    writefile.close()\n",
    "    print('end')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process tip.json\n",
    "header = 'user_id@compliment_count\\n'\n",
    "inputFile = '/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/tip.json'\n",
    "outputFilePath = \"/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/tip/\"\n",
    "outputFilePrefix = \"tips_\"\n",
    "fields = ['.user_id','.compliment_count']\n",
    "extractToFiles(header, inputFile, outputFilePath, outputFilePrefix, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process user.json\n",
    "header = 'user_id@user_name\\n'\n",
    "inputFile = '/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/usero.json'\n",
    "outputFilePath = \"/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/\"\n",
    "outputFilePrefix = \"userids_\"\n",
    "fields = ['.user_id', '.name']\n",
    "extractToFiles(header, inputFile, outputFilePath, outputFilePrefix, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Process review.json\n",
    "header = 'review_id@user_id@business_id@rating\\n'\n",
    "inputFile = '/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/reviewo.json'\n",
    "outputFilePath = \"/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/review/\"\n",
    "outputFilePrefix = \"reviews_\"\n",
    "fields = ['.review_id', '.user_id', '.business_id','.stars']\n",
    "extractToFiles(header, inputFile, outputFilePath, outputFilePrefix, fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process business.json\n",
    "header = 'business_id@name@categories\\n'\n",
    "inputFile = '/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/businesso.json'\n",
    "outputFilePath = \"/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/business/\"\n",
    "outputFilePrefix = \"business_\"\n",
    "fields = ['.business_id', '.name', '.categories']\n",
    "extractToFiles(header, inputFile, outputFilePath, outputFilePrefix, fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extractDfFromFile(nFiles, filePrefix, headerFlag):\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(1,nFiles+1): # loops from 1 to nFiles\n",
    "        if headerFlag == 0:\n",
    "            entries = pd.read_csv(filePrefix+str(i)+'.txt', sep = '@', header = None)\n",
    "        else:\n",
    "            entries = pd.read_csv(filePrefix+str(i)+'.txt', sep = '@')\n",
    "        df = df.append(pd.DataFrame(entries))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "# tip data: Build user id, compliment_count df from extracted tip data \n",
    "filePath = '/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/'\n",
    "filePrefix = filePath+'tips_'\n",
    "tipsDf = extractDfFromFile(13, filePrefix, 1)\n",
    "\n",
    "#aggregate compliment count\n",
    "tipsDf = tipsDf.groupby(['user_id']).sum()\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "# user data: Build user id, user name df from extracted user data \n",
    "filePath = '/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/'\n",
    "filePrefix = filePath+'userids_'\n",
    "usersDf = extractDfFromFile(17, filePrefix, 0)\n",
    "\n",
    "usersDf = usersDf.rename(columns={0: \"user_id\", 1: \"user_name\"})\n",
    "#print(usersDf.head())\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  user_id user_name  compliment_count\n",
      "0  4XChL029mKr5hydo79Ljxg     Jenna                 0\n",
      "1  bc8C_eETBWL0olvFSJJd0w     David                 0\n",
      "2  MM4RJAeH6yuaN8oZDSt0RA     Nancy                 0\n",
      "3  0rK89TS8xqy1wI4nYI1wfw   Marilyn                 0\n",
      "4  T0gWkTHWRChVUe_Dn1F8nw     Tanya                 0\n"
     ]
    }
   ],
   "source": [
    "#Merge user and tip and dataframes\n",
    "user_tipDf = pd.merge(usersDf, tipsDf, on='user_id', how = 'inner', sort = False, validate = 'one_to_one' )\n",
    "print(user_tipDf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "# review data: Build review id, userid, businessid df from extracted review data \n",
    "filePath = '/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/review/'\n",
    "filePrefix = filePath+'reviews_'\n",
    "reviewsDf = extractDfFromFile(67, filePrefix, 1)\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "#business data: business id, categories\n",
    "filePath = '/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/business/'\n",
    "filePrefix = filePath+'business_'\n",
    "businessDf = extractDfFromFile(2, filePrefix, 1)\n",
    "#print(businessDf.head())\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vcroopana/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "categories = businessDf['categories']\n",
    "catSet = set()\n",
    "for cat in categories:\n",
    "    catSet.update(set(cat.split(', ')))\n",
    "print(len(catSet))\n",
    "\n",
    "def countCats(categoriesRow):\n",
    "    currCat = categoriesRow['category']\n",
    "    business_matching_currCategory = businessDf[ businessDf['categories'].str.contains(currCat+\",\")]\n",
    "    busCount = business_matching_currCategory['business_id'].unique().size\n",
    "    return busCount\n",
    "\n",
    "categoriesDF = pd.DataFrame(columns = ['category', 'businessCount'])\n",
    "categoriesDF['category'] = list(catSet)\n",
    "categoriesDF['businessCount'] = np.zeros(len(categoriesDF['category']))\n",
    "categoriesDF['businessCount'] = categoriesDF.apply(lambda x:  countCats(x), axis = 1)\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categoriesDF.sort_values(by = 'businessCount', ascending=False, inplace=True)\n",
    "print(categoriesDF.head(10))\n",
    "outputFilePath = \"/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/business/\"\n",
    "categoriesDF.to_csv(outputFilePath+'categories'+\".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updated Categories\n",
    "outputFilePath = \"/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/business/\"\n",
    "categoriesDFUpdated = pd.read_csv(outputFilePath+'categories_updated'+\".csv\")\n",
    "\n",
    "categoriesDict = {}\n",
    "nCatsOriginal = categoriesDFUpdated['category'].size\n",
    "for i in range(0, nCatsOriginal):\n",
    "    categoriesDict[categoriesDFUpdated.iloc[i]['category']] = categoriesDFUpdated.iloc[i]['newCategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "#create all category cols in business df\n",
    "uniqueNewCategories = categoriesDFUpdated['newCategory'].unique()\n",
    "for newCat in uniqueNewCategories:\n",
    "    businessDf[newCat] = False\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234.90339014799997\n"
     ]
    }
   ],
   "source": [
    "# adds new column 'newCategories' in business dataframe\n",
    "import timeit\n",
    "import pandas as pd\n",
    "\n",
    "def updateEachCatDict(business):\n",
    "    currCats = business['categories'].split(', ')\n",
    "    updatedCategory = \"\"\n",
    "    i = 0\n",
    "    for cat in currCats:\n",
    "        updatedSingleCategory = categoriesDict[cat]\n",
    "        business[updatedSingleCategory] = True\n",
    "        if i == 0:\n",
    "            updatedCategory = categoriesDict[cat]\n",
    "        else:\n",
    "            value = categoriesDict[cat]\n",
    "            if value.casefold() not in updatedCategory.casefold():\n",
    "                updatedCategory = updatedCategory + \", \" + value\n",
    "        i = i+1\n",
    "    business['newCategories'] = updatedCategory\n",
    "    return business\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "businessDf['newCategories'] = ''\n",
    "businessDf = businessDf.apply(lambda x:  updateEachCatDict(x), axis = 1)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# businessDf = businessDf[['business_id', 'name', 'categories', 'newCategories']]\n",
    "# print(businessDf.head(20))\n",
    "# print(businessDf['newCategoriesDict'].equals(businessDf['newCategories'] ))\n",
    "# businessDf = businessDf[['business_id', 'name', 'categories', 'newCategories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "review_businessDf = pd.merge(reviewsDf, businessDf, on='business_id', how = 'left', sort = False)\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueNewCategories = categoriesDFUpdated['newCategory'].unique()\n",
    "\n",
    "user_cat_df = pd.DataFrame()\n",
    "user_cat_df['user_id'] = review_businessDf['user_id'].unique()\n",
    "print('Total no of users:'+ str(user_cat_df['user_id'].size))\n",
    "\n",
    "for cat in uniqueNewCategories:\n",
    "    \n",
    "    userGroups = review_businessDf[review_businessDf[cat] == True].groupby(['user_id'])\n",
    "    print(userGroups)\n",
    "    catUserRatings = userGroups['rating'].mean()\n",
    "    print('Size of users in category, '+ cat + \":\" + str(catUserRatings.size))\n",
    "    catUserRatings = pd.DataFrame({'user_id':catUserRatings.index, cat: catUserRatings.values})\n",
    "    user_cat_df = pd.merge(user_cat_df, catUserRatings, on ='user_id', how = 'left', sort = False )\n",
    "\n",
    "print(user_cat_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cat_matrix = user_cat_df.values\n",
    "print(user_cat_matrix)\n",
    "user_user = user_cat_matrix.dot(user_cat_matrix.transpose())\n",
    "print(user_user)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
