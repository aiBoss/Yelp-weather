{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extractDfFromFile(nFiles, filePrefix, headerFlag):\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(1,nFiles+1): # loops from 1 to nFiles\n",
    "        if headerFlag == 0:\n",
    "            entries = pd.read_csv(filePrefix+str(i)+'.txt', sep = '@', header = None)\n",
    "        else:\n",
    "            entries = pd.read_csv(filePrefix+str(i)+'.txt', sep = '@')\n",
    "        df = df.append(pd.DataFrame(entries))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "# tip data: Build user id, compliment_count df from extracted tip data \n",
    "filePath = '/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/'\n",
    "filePrefix = filePath+'tips_'\n",
    "tipsDf = extractDfFromFile(13, filePrefix, 1)\n",
    "\n",
    "#aggregate compliment count\n",
    "tipsDf = tipsDf.groupby(['user_id']).sum()\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "# user data: Build user id, user name df from extracted user data \n",
    "filePath = '/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/'\n",
    "filePrefix = filePath+'userids_'\n",
    "usersDf = extractDfFromFile(17, filePrefix, 0)\n",
    "\n",
    "usersDf = usersDf.rename(columns={0: \"user_id\", 1: \"user_name\"})\n",
    "#print(usersDf.head())\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  user_id user_name  compliment_count\n",
      "0  4XChL029mKr5hydo79Ljxg     Jenna                 0\n",
      "1  bc8C_eETBWL0olvFSJJd0w     David                 0\n",
      "2  MM4RJAeH6yuaN8oZDSt0RA     Nancy                 0\n",
      "3  0rK89TS8xqy1wI4nYI1wfw   Marilyn                 0\n",
      "4  T0gWkTHWRChVUe_Dn1F8nw     Tanya                 0\n"
     ]
    }
   ],
   "source": [
    "#Merge user and tip and dataframes\n",
    "user_tipDf = pd.merge(usersDf, tipsDf, on='user_id', how = 'inner', sort = False, validate = 'one_to_one' )\n",
    "print(user_tipDf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "# review data: Build review id, userid, businessid df from extracted review data \n",
    "filePath = '/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/review/'\n",
    "filePrefix = filePath+'reviews_'\n",
    "reviewsDf = extractDfFromFile(67, filePrefix, 1)\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "#business data: business id, categories\n",
    "filePath = '/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/business/'\n",
    "filePrefix = filePath+'business_'\n",
    "businessDf = extractDfFromFile(2, filePrefix, 1)\n",
    "#print(businessDf.head())\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vcroopana/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "categories = businessDf['categories']\n",
    "catSet = set()\n",
    "for cat in categories:\n",
    "    catSet.update(set(cat.split(', ')))\n",
    "print(len(catSet))\n",
    "\n",
    "def countCats(categoriesRow):\n",
    "    currCat = categoriesRow['category']\n",
    "    business_matching_currCategory = businessDf[ businessDf['categories'].str.contains(currCat+\",\")]\n",
    "    busCount = business_matching_currCategory['business_id'].unique().size\n",
    "    return busCount\n",
    "\n",
    "categoriesDF = pd.DataFrame(columns = ['category', 'businessCount'])\n",
    "categoriesDF['category'] = list(catSet)\n",
    "categoriesDF['businessCount'] = np.zeros(len(categoriesDF['category']))\n",
    "categoriesDF['businessCount'] = categoriesDF.apply(lambda x:  countCats(x), axis = 1)\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categoriesDF.sort_values(by = 'businessCount', ascending=False, inplace=True)\n",
    "# print(categoriesDF.head(10))\n",
    "# outputFilePath = \"/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/business/\"\n",
    "# categoriesDF.to_csv(outputFilePath+'categories'+\".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dic of updated Categories\n",
    "outputFilePath = \"/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/business/\"\n",
    "categoriesDFUpdated = pd.read_csv(outputFilePath+'categories_updated'+\".csv\")\n",
    "\n",
    "categoriesDict = {}\n",
    "nCatsOriginal = categoriesDFUpdated['category'].size\n",
    "for i in range(0, nCatsOriginal):\n",
    "    categoriesDict[categoriesDFUpdated.iloc[i]['category']] = categoriesDFUpdated.iloc[i]['newCategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "#create all category cols in business df\n",
    "uniqueNewCategories = categoriesDFUpdated['newCategory'].unique()\n",
    "for newCat in uniqueNewCategories:\n",
    "    businessDf[newCat] = False\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307.789729781\n"
     ]
    }
   ],
   "source": [
    "# adds new column 'newCategories' in business dataframe\n",
    "import timeit\n",
    "import pandas as pd\n",
    "\n",
    "def updateEachCatDict(business):\n",
    "    currCats = business['categories'].split(', ')\n",
    "    updatedCategory = \"\"\n",
    "    i = 0\n",
    "    for cat in currCats:\n",
    "        updatedSingleCategory = categoriesDict[cat]\n",
    "        business[updatedSingleCategory] = True\n",
    "        if i == 0:\n",
    "            updatedCategory = categoriesDict[cat]\n",
    "        else:\n",
    "            value = categoriesDict[cat]\n",
    "            if value.casefold() not in updatedCategory.casefold():\n",
    "                updatedCategory = updatedCategory + \", \" + value\n",
    "        i = i+1\n",
    "    business['newCategories'] = updatedCategory\n",
    "    return business\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "businessDf['newCategories'] = ''\n",
    "businessDf = businessDf.apply(lambda x:  updateEachCatDict(x), axis = 1)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "review_businessDf = pd.merge(reviewsDf, businessDf, on='business_id', how = 'left', sort = False)\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueNewCategories = categoriesDFUpdated['newCategory'].unique()\n",
    "\n",
    "user_cat_df = pd.DataFrame()\n",
    "user_cat_df['user_id'] = review_businessDf['user_id'].unique()\n",
    "print('Total no of users:'+ str(user_cat_df['user_id'].size))\n",
    "\n",
    "for cat in uniqueNewCategories:\n",
    "    \n",
    "    userGroups = review_businessDf[review_businessDf[cat] == True].groupby(['user_id'])\n",
    "    print(userGroups)\n",
    "    catUserRatings = userGroups['rating'].mean()\n",
    "    print('Size of users in category, '+ cat + \":\" + str(catUserRatings.size))\n",
    "    catUserRatings = pd.DataFrame({'user_id':catUserRatings.index, cat: catUserRatings.values})\n",
    "    user_cat_df = pd.merge(user_cat_df, catUserRatings, on ='user_id', how = 'left', sort = False )\n",
    "\n",
    "print(user_cat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1637138, 34)\n",
      "[[2.2        2.         1.         ... 1.         0.         2.33333333]\n",
      " [3.         0.         0.         ... 0.         0.         3.        ]\n",
      " [5.         5.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [5.         0.         0.         ... 0.         0.         0.        ]\n",
      " [5.         0.         0.         ... 0.         0.         5.        ]\n",
      " [5.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(user_cat_df.shape)\n",
    "user_cat_df_0 = user_cat_df.fillna(0)\n",
    "user_cat_0_matrix = user_cat_df_0.drop(['user_id'], axis = 1).values\n",
    "print(user_cat_0_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n",
      "(1637138, 34)\n"
     ]
    }
   ],
   "source": [
    "#user_user = user_cat_0_matrix.dot(user_cat_0_matrix.transpose())\n",
    "print('end')\n",
    "#print(user_user)\n",
    "print(user_cat_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFilePath = \"/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/business/\"\n",
    "user_cat_df_0.to_csv(outputFilePath + 'user_category'+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(user_cat_df_0.isin([0]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.sparse import csr_matrix\n",
    "\n",
    "# sparseMatrix = csr_matrix(user_cat_0_matrix)\n",
    "# user_user_sparse = sparseMatrix.dot(csr_matrix(user_cat_0_matrix.transpose()))\n",
    "# print(user_user_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "outputFilePath = \"/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/business/\"\n",
    "df = pd.read_csv(outputFilePath+'user_category'+\".csv\")\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "2\n",
      "300256\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing as pp\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.linalg.blas\n",
    "\n",
    "def cosine_similarities(mat):\n",
    "\n",
    "    row_normed_mat = pp.normalize(csr_matrix(mat), axis=1) # 1 => normalizes each row, doing this for cosine similarity\n",
    "    #nRows = row_normed_mat.shape[0]\n",
    "    nRows = 1\n",
    "    count = 1\n",
    "    filecount = 1\n",
    "    outputFilePath = \"/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/business/\"\n",
    "    outputFilePrefix = 'user_user_'\n",
    "    writefile= open(outputFilePath+outputFilePrefix+\"1.txt\",\"w+\")\n",
    "    header = 'user_index@most_similar_user_index'\n",
    "    writefile.writelines(header)\n",
    "    maxInd =0\n",
    "    for i in range(0, nRows):\n",
    "        #val = row_normed_mat.getrow(i) * row_normed_mat.T\n",
    "        \n",
    "        val = scipy.linalg.blas.sgemm(alpha=1.0, a= row_normed_mat.getrow(i).toarray(), b= row_normed_mat.toarray(), trans_b=True)\n",
    "        #maxInd = np.argsort(val.getrow(0).toarray().flatten())[-2]\n",
    "        maxInd = np.argsort(val.flatten())[-2]\n",
    "        if count == (filecount * 100000): # change to new file for every 1 lakh rows\n",
    "            writefile.close()\n",
    "            filecount = filecount+1\n",
    "            print(\"count: \"+ str(count) + \" curr filecount: \"+ str(filecount))\n",
    "            writefile= open(outputFilePath+outputFilePrefix+str(filecount)+\".txt\",\"w+\")       \n",
    "            writefile.writelines(header)\n",
    "        \n",
    "        writefile.writelines(str(i)+ \"@\" + str(maxInd))\n",
    "        count = count+1\n",
    "        print(count)\n",
    "    writefile.close()\n",
    "    return maxInd\n",
    "    #return row_normed_mat * row_normed_mat.T\n",
    "\n",
    "print('started')\n",
    "vals = df.drop(['user_id'], axis = 1).to_numpy(dtype= 'float32')\n",
    "cosineSims = cosine_similarities(vals)\n",
    "\n",
    "print(cosineSims)\n",
    "\n",
    "# use triangular matrix \n",
    "# or use 1 by 16 lakhs\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rowStartInd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-549d5931324c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblockShape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblockStartInd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mblockEndInd\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mactualInd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrowStartInd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mactualInd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblockStartInd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblockEndInd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rowStartInd' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.metrics.pairwise import linear_kernel \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix, csc_matrix, lil_matrix\n",
    "\n",
    "vals = df.drop(['user_id'], axis = 1).to_numpy(dtype= 'float32')\n",
    "nRows = 1637138\n",
    "blockSize = 1000\n",
    "nBlocks = 1630\n",
    "\n",
    "\n",
    "filecount = 1\n",
    "outputFilePath = \"/Users/vcroopana/Downloads/Spring2020/BigData/Proj1_No_SQL/yelp_dataset/business/\"\n",
    "outputFilePrefix = 'user_user_'\n",
    "writefile= open(outputFilePath+outputFilePrefix+\"1.txt\",\"w+\")\n",
    "header = 'user_index@most_similar_user_index@value\\n'\n",
    "writefile.writelines(header)\n",
    "\n",
    "for i in range(999, nRows):\n",
    "    maxInd =0\n",
    "    maxVal =0\n",
    "    count = i + 1\n",
    "    for j in range(0 , nBlocks):\n",
    "        if j == 0:\n",
    "            blockStartInd = j*blockSize\n",
    "        else:\n",
    "            blockStartInd = (j*blockSize) + 1\n",
    "        \n",
    "        blockEndInd = (j+1)*blockSize\n",
    "        blockShape = csr_matrix(vals[blockStartInd:blockEndInd,:]).shape\n",
    "        if(blockShape[0]!=0):\n",
    "            res = cosine_similarity(csr_matrix(vals[i]), csr_matrix(vals[blockStartInd:blockEndInd ,:]))\n",
    "            actualInd = (rowStartInd + i)\n",
    "                \n",
    "            if actualInd in range(blockStartInd,blockEndInd+1):\n",
    "                res[0, i] = 0\n",
    "                \n",
    "            #tempMaxInd = np.maximum(res.flatten())[-2] #tempMaxVal = res.flatten()[tempMaxInd] #tempMaxVal = res.max()\n",
    "            tempMaxInd = np.argmax(res[0])\n",
    "            tempMaxVal = res[0, tempMaxInd]\n",
    "            tempMaxInd = blockStartInd - 1 + tempMaxInd\n",
    "            if(tempMaxVal> maxVal):\n",
    "                maxInd = tempMaxInd\n",
    "                maxVal = tempMaxVal\n",
    "    if count == (filecount * 100000): # change to new file for every 1 lakh rows\n",
    "        writefile.close()\n",
    "        filecount = filecount+1\n",
    "        print(\"count: \"+ str(count) + \" curr filecount: \"+ str(filecount))\n",
    "        writefile= open(outputFilePath+outputFilePrefix+str(filecount)+\".txt\",\"w+\")       \n",
    "        writefile.writelines(header)\n",
    "    writefile.writelines(str(i)+\"@\"+str(maxInd)+\"@\"+str(maxVal)+\"\\n\")\n",
    "    print(str(i)+\"@\"+str(maxInd)+\"@\"+str(maxVal))\n",
    "    if(i%100 ==0):\n",
    "        print(str(i)+\" maxInd:\"+ str(maxInd)+\"@\"+str(maxVal))\n",
    "\n",
    "writefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tables\n",
    "# import time\n",
    "\n",
    "# h5f_C.close()\n",
    "# A = user_cat_0_matrix\n",
    "# B = user_cat_0_matrix.transpose()\n",
    "\n",
    "# fileName_C = outputFilePath+ 'CArray_C.h5'\n",
    "# atom = tables.Float32Atom()\n",
    "# nUsers = user_cat_df_0.shape[0]\n",
    "# shape = (nUsers, nUsers)\n",
    "# nChunk = 500 \n",
    "# chunk_shape = (nChunk, nChunk)\n",
    "# chunk_multiple = 1 #?\n",
    "# block_size = chunk_multiple * nChunk\n",
    "# h5f_C = tables.open_file(fileName_C, 'w')\n",
    "# C = h5f_C.create_carray(h5f_C.root, 'CArray', atom, shape, chunkshape=chunk_shape)\n",
    "\n",
    "# sz= block_size\n",
    "\n",
    "# t0= time.time()\n",
    "# for i in range(0, A.shape[0], sz):\n",
    "#     for j in range(0, B.shape[1], sz):\n",
    "#         for k in range(0, A.shape[1], sz):\n",
    "#             C[i:i+sz,j:j+sz] += np.dot(A[i:i+sz,k:k+sz],B[k:k+sz,j:j+sz])\n",
    "# print (time.time()-t0)\n",
    "\n",
    "# t0 = time.time()\n",
    "# res = np.dot(A,B)\n",
    "# print (time.time()-t0)\n",
    "\n",
    "# print (C == res)\n",
    "\n",
    "# h5f_C.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
